{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dame-cell/Gaja/blob/main/training_the_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# steps followed for the full-fine tuning:\n",
        "\n",
        "1. First we import and install the dependencies\n",
        "2. second we load the configuration which makes life easier\n",
        "3. mount goggle drive so that we can store our checkpoints\n",
        "4. downloading the base-model using unsloth and in 4-bit\n",
        "5. Get the peft-configuration\n",
        "6. load the datasets and also pre-precoess it a bit\n",
        "7. and finally truly starting the training\n",
        "\n"
      ],
      "metadata": {
        "id": "lvh1aTsUk2M0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iyxl9bXHmfPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJhPsHvWyPsS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install \"unsloth[colab_ampere] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "pass\n",
        "\n",
        "!pip install \"git+https://github.com/huggingface/transformers.git\" # Native 4bit loading works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9aeCDOWxJp7"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# Specify the path to your YAML configuration file\n",
        "config_file_path = \"config.yaml\"\n",
        "\n",
        "# Load the YAML file\n",
        "with open(config_file_path, \"r\") as config_file:\n",
        "    config = yaml.safe_load(config_file)\n",
        "\n",
        "# Access all configuration parameters\n",
        "max_seq_length = config.get('max_seq_length')\n",
        "dtype = config.get('dtype')\n",
        "load_in_4bit = config.get('load_in_4bit')\n",
        "model_name = config.get('model_name')\n",
        "target_modules = config.get('target_modules')\n",
        "ra_alpha = config.get('ra_alpha')\n",
        "lora_dropout = config.get('lora_dropout')\n",
        "bias = config.get('bias')\n",
        "use_gradient_checkpointing = config.get('use_gradient_checkpointing')\n",
        "random_state = config.get('random_state')\n",
        "use_rslora = config.get('use_rslora')\n",
        "loftq_config = config.get('loftq_config')\n",
        "dataset_text_field = config.get('dataset_text_field')\n",
        "dataset_num_proc = config.get('ataset_num_proc')  # Typo in the original config, fix it to 'dataset_num_proc'\n",
        "packing = config.get('packing')\n",
        "per_device_train_batch_size = config.get('per_device_train_batch_size')\n",
        "gradient_accumulation_steps = config.get('gradient_accumulation_steps')\n",
        "warmup_steps = config.get('warmup_steps')\n",
        "hub_strategy = config.get('hub_strategy')\n",
        "num_train_epochs = config.get('num_train_epochs')\n",
        "push_to_hub = config.get('push_to_hub')\n",
        "push_to_hub_model_id = config.get('push_to_hub_model_id')\n",
        "learning_rate = config.get('learning_rate')\n",
        "resume_from_checkpoint = config.get('resume_from_checkpoint')\n",
        "\n",
        "logging_steps = config.get('logging_steps')\n",
        "optim = config.get('optim')\n",
        "weight_decay = config.get('weight_decay')\n",
        "save_total_limit = config.get('save_total_limit')\n",
        "save_steps = config.get('save_steps')\n",
        "lr_scheduler_type = config.get('lr_scheduler_type')\n",
        "seed = config.get('seed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCFkBz3LxUah"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDq__xLlyg9D"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMFsxahRzJQi"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bP0eu5vzZ7M"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"damerajee/insutrct-vls\",split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvZT7A992nCb"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBmorGkqzpBj"
      },
      "outputs": [],
      "source": [
        "data= dataset.remove_columns('Unnamed: 0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKRlkGPr2tXq"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76S_hGzj4cFU"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI7lU7JwzXQq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = data.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN-refkC2aez"
      },
      "outputs": [],
      "source": [
        "dataset['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLUs5o4_2acL"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U6dNF2m3OZj"
      },
      "outputs": [],
      "source": [
        "output_dir = \"output-path\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05WiZMbexssw"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjwfHx-M3IEE"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = dataset_text_field,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = dataset_num_proc,\n",
        "    packing = packing, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = per_device_train_batch_size,\n",
        "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "        warmup_steps = warmup_steps,\n",
        "        hub_strategy=hub_strategy,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        push_to_hub=push_to_hub,\n",
        "        push_to_hub_model_id=push_to_hub_model_id,\n",
        "        learning_rate = 2e-4,\n",
        "        resume_from_checkpoint=resume_from_checkpoint,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = logging_steps,\n",
        "        optim = optim,\n",
        "        weight_decay = weight_decay,\n",
        "        save_total_limit=save_total_limit,\n",
        "        save_steps=save_steps,\n",
        "        lr_scheduler_type = lr_scheduler_type,\n",
        "        seed = seed,\n",
        "\n",
        "        output_dir=output_dir,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some-cool-information\n",
        "\n",
        "1. First,you can easily run all teh code and try it yourselves\n",
        "2. the max memory taken before training was 4.3 gb and after was almost 8gb\n"
      ],
      "metadata": {
        "id": "boWwuuNQlxY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NGYbtqoM4DPr"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7DQHlCv18QZ"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train(\"checkpoint-path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZqb5eAPqDtY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN0V0odoDK1u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaAhXmofYQy/etczCpwtzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}